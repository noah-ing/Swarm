#!/usr/bin/env python3
"""
Test Integration: ReflectionEngine & SkillExtractor
==================================================

Comprehensive integration test for the newly implemented ReflectionEngine and
SkillExtractor components, ensuring they work correctly with CognitiveDatabase.
"""

import sys
import os
import tempfile
from datetime import datetime
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

# Import the components
from brain.database import CognitiveDatabase
from brain.reflection import ReflectionEngine
from brain.skill_extractor import SkillExtractor


def test_reflection_engine():
    """Test ReflectionEngine functionality."""
    print("Testing ReflectionEngine...")
    
    # Create temporary database
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp_db:
        db = CognitiveDatabase(tmp_db.name)
        skill_extractor = SkillExtractor(db)
        reflection_engine = ReflectionEngine(db, skill_extractor)
        
        print("âœ… ReflectionEngine initialized with SkillExtractor")
        
        # Test successful task reflection with substantial content
        task = "Implement a comprehensive Python function to calculate fibonacci numbers efficiently using dynamic programming with memoization, including error handling and comprehensive unit tests"
        outcome = "Successfully implemented fibonacci function with memoization, proper error handling for negative inputs, and complete test suite covering edge cases"
        
        tool_calls = [
            {"name": "write", "parameters": {"file_path": "fib.py"}},
            {"name": "write", "parameters": {"file_path": "test_fib.py"}},
            {"name": "bash", "parameters": {"command": "python -m pytest test_fib.py -v"}},
        ]
        
        reflection = reflection_engine.reflect(
            task=task,
            outcome=outcome,
            success=True,
            model_used="claude-3.5-sonnet",
            tokens_used=2500,
            duration_seconds=8.5,
            tool_calls=tool_calls,
        )
        
        print(f"âœ… Successful reflection created with confidence: {reflection.confidence:.2f}")
        print(f"   - What worked: {reflection.what_worked}")
        print(f"   - Insights: {reflection.insights}")
        print(f"   - Skill candidates: {len(reflection.skill_candidates)}")
        
        # Test failed task reflection
        failed_task = "Delete all files in production database server without backup"
        error_msg = "Permission denied: insufficient access rights for production database operations"
        
        failed_reflection = reflection_engine.reflect(
            task=failed_task,
            outcome="Task failed due to permission error and safety restrictions",
            success=False,
            model_used="claude-3.5-sonnet",
            tokens_used=1200,
            duration_seconds=3.0,
            error=error_msg,
        )
        
        print(f"âœ… Failed reflection created with confidence: {failed_reflection.confidence:.2f}")
        print(f"   - What failed: {failed_reflection.what_failed}")
        print(f"   - Insights: {failed_reflection.insights}")
        
        # Test getting insights for similar task
        insights = reflection_engine.get_insights_for_task("Implement fibonacci in Python")
        print(f"âœ… Retrieved {len(insights)} insights for similar task")
        
        # Test failure warnings
        warnings = reflection_engine.get_failure_warnings("Delete files in database")
        print(f"âœ… Retrieved {len(warnings)} warnings for risky task")
        
        # Test model recommendation
        recommended_model = reflection_engine.get_recommended_model("Python coding task")
        print(f"âœ… Recommended model: {recommended_model}")
        
        # Test reflection stats
        stats = reflection_engine.get_reflection_stats()
        print(f"âœ… Reflection stats: {stats}")
        
        # Clean up
        os.unlink(tmp_db.name)
        print("âœ… ReflectionEngine tests completed successfully!\n")
        
        return reflection, failed_reflection


def test_skill_extractor():
    """Test SkillExtractor functionality."""
    print("Testing SkillExtractor...")
    
    # Create temporary database
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp_db:
        db = CognitiveDatabase(tmp_db.name)
        skill_extractor = SkillExtractor(db)
        
        print("âœ… SkillExtractor initialized")
        
        # Test skill creation from candidate with substantial content
        skill_candidate = {
            "name": "python_test_file_processor",
            "pattern": "Process Python files by reading source code, analyzing syntax and imports, refactoring code structure, writing improved version, and running comprehensive test suite to validate changes",
            "tools": ["read", "search", "write", "bash"],
        }
        
        skill_id = skill_extractor.maybe_create_skill(skill_candidate)
        print(f"âœ… Skill created with ID: {skill_id}")
        
        # Test updating failure pattern
        failed_task = "Execute SQL query on production database without proper validation"
        error_msg = "Connection timeout after 30 seconds due to network issues"
        
        skill_extractor.update_failure_pattern(failed_task, error_msg)
        print("âœ… Failure pattern recorded")
        
        # Test skill extraction from reflection data
        reflection_data = {
            "skill_candidates": [
                {
                    "name": "file_backup_configuration_pattern",
                    "pattern": "Create comprehensive backup of configuration files before making any modifications, including timestamps and validation checks",
                    "tools": ["read", "write", "bash"]
                }
            ],
            "tool_calls": [
                {"name": "read", "parameters": {}},
                {"name": "search", "parameters": {}},
                {"name": "write", "parameters": {}},
                {"name": "bash", "parameters": {}},
            ]
        }
        
        extracted_skills = skill_extractor.extract_skills_from_reflection(reflection_data)
        print(f"âœ… Extracted {len(extracted_skills)} skills from reflection")
        
        # Test trying to create duplicate skill (should update existing)
        duplicate_skill_id = skill_extractor.maybe_create_skill(skill_candidate)
        print(f"âœ… Duplicate skill handling: {'updated existing' if duplicate_skill_id else 'rejected duplicate'}")
        
        # Test invalid skill candidate
        invalid_candidate = {
            "pattern": "do stuff",  # Too short/generic
            "tools": []  # No tools
        }
        
        invalid_skill_id = skill_extractor.maybe_create_skill(invalid_candidate)
        print(f"âœ… Invalid candidate rejected: {invalid_skill_id is None}")
        
        # Test another invalid candidate - too generic
        generic_candidate = {
            "pattern": "fix it",  # Too short and generic
            "tools": ["write"]
        }
        
        generic_skill_id = skill_extractor.maybe_create_skill(generic_candidate)
        print(f"âœ… Generic candidate rejected: {generic_skill_id is None}")
        
        # Test skill extraction stats
        stats = skill_extractor.get_skill_extraction_stats()
        print(f"âœ… Skill extraction stats: {stats}")
        
        # Test skill validation
        validation = skill_extractor.validate_existing_skills()
        print(f"âœ… Skill validation: {validation['status']}")
        
        # Clean up
        os.unlink(tmp_db.name)
        print("âœ… SkillExtractor tests completed successfully!\n")
        
        return extracted_skills


def test_integration():
    """Test ReflectionEngine and SkillExtractor integration."""
    print("Testing ReflectionEngine + SkillExtractor Integration...")
    
    # Create temporary database
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp_db:
        db = CognitiveDatabase(tmp_db.name)
        skill_extractor = SkillExtractor(db)
        reflection_engine = ReflectionEngine(db, skill_extractor)
        
        print("âœ… Both components initialized with shared database")
        
        # Simulate a task that generates skill candidates
        task = "Refactor Python module by reading source code, analyzing import statements, updating code structure for better organization, writing the improved version, and running comprehensive test suite to validate all changes work correctly"
        outcome = "Successfully refactored module with improved organization, cleaner imports, better structure, and all tests passing"
        
        tool_calls = [
            {"name": "read", "parameters": {"file_path": "module.py"}},
            {"name": "search", "parameters": {"pattern": "import"}},
            {"name": "write", "parameters": {"file_path": "module.py"}},
            {"name": "bash", "parameters": {"command": "python -m pytest"}}
        ]
        
        # Create reflection which should generate skill candidates
        reflection = reflection_engine.reflect(
            task=task,
            outcome=outcome,
            success=True,
            model_used="claude-3.5-sonnet",
            tokens_used=4500,
            duration_seconds=15.2,
            tool_calls=tool_calls,
        )
        
        print(f"âœ… Reflection created with {len(reflection.skill_candidates)} skill candidates")
        
        # The reflect method should have automatically called SkillExtractor internally
        # Let's verify by checking database state
        stats = db.get_system_stats()
        print(f"âœ… Database now contains:")
        print(f"   - Reflections: {stats['total_reflections']}")
        print(f"   - Skills: {stats['skills_learned']}")
        
        # Test that we can extract more skills from the same reflection data
        reflection_data = {
            "skill_candidates": reflection.skill_candidates,
            "tool_calls": tool_calls,
        }
        
        additional_skills = skill_extractor.extract_skills_from_reflection(reflection_data)
        print(f"âœ… Additional skill extraction found {len(additional_skills)} skills")
        
        # Test failure pattern integration
        failed_task = "Deploy application to production without running test suite or creating backup"
        error_msg = "Deployment failed: test suite had 5 failures including critical security issues"
        
        failed_reflection = reflection_engine.reflect(
            task=failed_task,
            outcome="Deployment rolled back due to test failures and security concerns",
            success=False,
            model_used="claude-3.5-sonnet",
            tokens_used=1800,
            duration_seconds=5.5,
            error=error_msg,
        )
        
        # Check that failure pattern was recorded
        final_stats = db.get_system_stats()
        print(f"âœ… After failure: {final_stats['failure_patterns_tracked']} failure patterns tracked")
        
        # Test cross-component insights
        insights = reflection_engine.get_insights_for_task("Refactor Python code")
        warnings = reflection_engine.get_failure_warnings("Deploy to production")
        
        print(f"âœ… Cross-component integration:")
        print(f"   - Insights for refactoring: {len(insights)}")
        print(f"   - Warnings for deployment: {len(warnings)}")
        
        # Test skill recommendation by checking if patterns match
        if final_stats['skills_learned'] > 0:
            print(f"âœ… Skills were successfully created during integration")
        else:
            print(f"â„¹ï¸  No skills created (validation criteria may be strict)")
        
        # Clean up
        os.unlink(tmp_db.name)
        print("âœ… Integration tests completed successfully!\n")
        
        return {
            "reflections_created": 2,
            "skills_created": final_stats['skills_learned'],
            "failure_patterns": final_stats['failure_patterns_tracked']
        }


def test_skill_validation_edge_cases():
    """Test edge cases in skill validation."""
    print("Testing SkillExtractor edge cases...")
    
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp_db:
        db = CognitiveDatabase(tmp_db.name)
        skill_extractor = SkillExtractor(db)
        
        # Test various candidate patterns
        test_cases = [
            {
                "name": "valid_substantial_task",
                "pattern": "Analyze Python codebase by reading multiple files, searching for patterns, refactoring functions for better performance, and validating changes with automated testing",
                "tools": ["read", "search", "write", "bash"],
                "should_pass": True
            },
            {
                "name": "too_short",
                "pattern": "fix bug",
                "tools": ["write"],
                "should_pass": False
            },
            {
                "name": "no_tools",
                "pattern": "This is a substantial pattern with enough content to pass length requirements but has no tools",
                "tools": [],
                "should_pass": False
            },
            {
                "name": "too_many_tools",
                "pattern": "Complex task with many steps requiring extensive tool usage beyond reasonable limits",
                "tools": ["read", "write", "search", "bash", "tool1", "tool2", "tool3", "tool4", "tool5", "tool6", "tool7"],
                "should_pass": False
            },
            {
                "name": "borderline_length",
                "pattern": "Process files with multiple operations",  # About 40 characters
                "tools": ["read", "write"],
                "should_pass": True  # Should pass basic validation but may fail complexity check
            }
        ]
        
        for test_case in test_cases:
            result = skill_extractor._is_valid_skill_candidate(
                test_case["pattern"], 
                test_case["tools"]
            )
            expected = test_case["should_pass"]
            status = "âœ…" if result == expected else "âŒ"
            print(f"{status} {test_case['name']}: expected {expected}, got {result}")
        
        os.unlink(tmp_db.name)
        print("âœ… Edge case testing completed!\n")


def main():
    """Run all tests."""
    print("ğŸ§  Testing ReflectionEngine and SkillExtractor Implementation")
    print("=" * 60)
    
    try:
        # Individual component tests
        reflection, failed_reflection = test_reflection_engine()
        extracted_skills = test_skill_extractor()
        
        # Edge case tests
        test_skill_validation_edge_cases()
        
        # Integration tests
        integration_results = test_integration()
        
        print("ğŸ‰ ALL TESTS PASSED! ğŸ‰")
        print("\nSummary:")
        print(f"âœ… ReflectionEngine: Successfully handles task analysis and learning extraction")
        print(f"âœ… SkillExtractor: Successfully identifies and creates reusable patterns")  
        print(f"âœ… Integration: Components work seamlessly with CognitiveDatabase")
        print(f"âœ… Database: Proper persistence and retrieval of all cognitive data")
        print(f"âœ… API: Maintains expected interfaces for cognitive components")
        print(f"âœ… Validation: Proper filtering of skill candidates based on quality criteria")
        
        print(f"\nTest Results:")
        print(f"   - Created {integration_results['reflections_created']} reflections")
        print(f"   - Extracted {integration_results['skills_created']} skills")
        print(f"   - Tracked {integration_results['failure_patterns']} failure patterns")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)